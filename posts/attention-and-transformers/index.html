<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="author" content="Fei Li"><meta name="description" content="Fei Li's data science and programming blogs."><meta name="keywords" content="mathematics,data science,machine learning,deep learning,artificial intelligence,scientific computing,programming,python,julia"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://lifeitech.github.io/assets/imgs/website-post.jpg"><meta property="og:title" content="lifeitech.github.io"><meta property="og:description" content="Fei Li's data science and programming blogs."><meta property="og:url" content="https://lifeitech.github.io"><meta property="og:type" content="article"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Attention and Transformers" /><meta property="og:locale" content="en" /><meta name="description" content="In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial on Transformer, and talk about several state of the arts models based on it, including GPT and BERT." /><meta property="og:description" content="In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial on Transformer, and talk about several state of the arts models based on it, including GPT and BERT." /><link rel="canonical" href="https://lifeitech.github.io/posts/attention-and-transformers/" /><meta property="og:url" content="https://lifeitech.github.io/posts/attention-and-transformers/" /><meta property="og:site_name" content="lifeitech.github.io" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-04-11T22:30:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Attention and Transformers" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-04-04T09:53:55+08:00","datePublished":"2022-04-11T22:30:00+08:00","description":"In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial on Transformer, and talk about several state of the arts models based on it, including GPT and BERT.","headline":"Attention and Transformers","mainEntityOfPage":{"@type":"WebPage","@id":"https://lifeitech.github.io/posts/attention-and-transformers/"},"url":"https://lifeitech.github.io/posts/attention-and-transformers/"}</script><title>Attention and Transformers | lifeitech.github.io</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/imgs/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/imgs/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/imgs/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/imgs/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/imgs/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="lifeitech.github.io"><meta name="application-name" content="lifeitech.github.io"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/imgs/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css"> <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/38937278" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">lifeitech.github.io</a></div><div class="site-subtitle">data science and programming blogs</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <span style="font-size: 1.2em;">üè†</span> <span>Home</span> </a><li class="nav-item"> <a href="/notes/" class="nav-link"> <span style="font-size: 1.2em;">üìï</span> <span>Notes</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <span style="font-size: 1.2em;">üìÅ</span> <span>Categories</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <span style="font-size: 1.2em;">üîñ</span> <span>Tags</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <span style="font-size: 1.2em;">üì¶</span> <span>Archives</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <span style="font-size: 1.2em;">‚ö°</span> <span>About</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://www.linkedin.com/in/lifeipro" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://github.com/lifeitech" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['fei.li.best','outlook.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Attention and Transformers</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@lifeitech"><meta name="twitter:creator" content="@lifeitech"><meta property="og:image" content="https://lifeitech.github.io/assets/imgs/transformers/transformers.jpg"><meta property="og:title" content="Attention and Transformers"><meta property="og:description" content="<blockquote><p>In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial on Transformer, and talk about several state of the arts models based on it, including GPT and BERT.</p></blockquote>"><meta property="og:url" content="https://lifeitech.github.io/posts/attention-and-transformers/"><meta property="og:type" content="article"> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 800 500'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/imgs/transformers/transformers.jpg" class="preview-img bg" alt="Attention and Transformers" width="800" height="500" ><h1 data-toc-skip>Attention and Transformers</h1><div class="post-meta text-muted"><div> By <em> <a href="https://lifeitech.github.io">Fei Li</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" date="2022-04-11 22:30:00 +0800" data-toggle="tooltip" data-placement="bottom" title="Mon, Apr 11, 2022, 10:30 PM +0800" >Apr 11, 2022</em> </span> <span> Updated <em class="timeago" date="2023-04-04 09:53:55 +0800 " data-toggle="tooltip" data-placement="bottom" title="Tue, Apr 4, 2023, 9:53 AM +0800" >Apr 4, 2023</em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1825 words"> <em>12 min</em> read</span></div></div></div><div class="post-content js-toc-content"><blockquote><p>In recent years, Transformer-based models are trending and are quickly taking up not only the field of NLP, but also computer vision and many other fields in AI. In this post, we give a tutorial on Transformer, and talk about several state of the arts models based on it, including GPT and BERT.</p></blockquote><h2 id="the-attention-model">The Attention Model<a href="#the-attention-model"><i class="fas fa-hashtag"></i></a></h2></h2><p>Even with <a href="/posts/rnn/#long-short-term-memories-lstm">LSTM</a> units, problems remain for recurrent neural network models:</p><ol><li>Passing data forward through an extended series of recurrent connections leads to loss of information and to difficulties in training.<li>Moreover, inherently sequential nature of recurrent networks inhibits the use of parallel computational resources.</ol><p>Attention <a href="#1">[1]</a> model (Figure 1) takes a different approach. Each output element $y_i$ in the output sequence $Y=y_1\cdots y_n$ is computed directly as a function of $x_1,\ldots,x_i$, the input sequence up to element $i$. Namely, $y_i$ is connected to $x_1,\ldots,x_i$ by <strong><em>direct</em></strong> connections. Thus, computation at $y_i$ is independent of computations for other outputs, so that they can be performed in parallel.</p><p><img data-proofer-ignore data-src="/assets/imgs/transformers/attention.png" alt="Self-Attention Layer" /> <em>Figure 1: Self-Attention Layer</em></p><p>Specifically, output $y_i$ at position $i$ is a weighted sum of input $i$ with all previous inputs, $x_1,\ldots,x_i$:</p>\[y_i = \alpha_{i1}x_1 + \alpha_{i2}x_2 + \cdots + \alpha_{ii}x_i.\]<p>The weight $\alpha_{ij}$ is a normalized score $s(x_i,x_j)$ for $x_i$ and $x_j$, reflecting their similarity, the simplest of which is the dot product. Specifically,</p>\[\alpha_{ij} = \frac{ \exp(s(x_i, x_j)) }{\sum\limits_{j'=1}^i\exp(s(x_i,x_{j'}))} \qquad \forall j\leq i.\]<p>To allow the possibility for learning, instead of summing raw input values in the above equation for calculating $y_i$, each input $x_i$ is first mapped to three vectors via three weight matrices:</p>\[\begin{align} k_i &amp;= W^Kx_i,\\ q_i &amp;= W^Qx_i,\\ v_i &amp;= W^Vx_i. \end{align}\]<p>And then,</p><ol><li><p>the key vector $k_j$ replaces $x_j$ in $s(x_i,\textcolor{red}{x_j})$ for $j\leq i$;</p><li><p>the query vector $q_i$ replaces $x_i$ in $s(\textcolor{red}{x_i},x_j)$;</p><li><p>the value vector $v_i$ replaces $x_i$ in</p></ol>\[y_i = \alpha_{i1}x_1 + \alpha_{i2}x_2 + \cdots + \alpha_{ii}\textcolor{red}{x_i}.\]<p>See figure below for an illustration when calculating $y_3$.</p><p><img data-proofer-ignore data-src="/assets/imgs/transformers/attention-detail.png" alt="Illustration for Self-Attention Layer; Transformer" /> <em>Figure 2: Illustration for calculating the third output element $y_3$ in an attention model.</em></p><h3 id="practical-considerations">Practical Considerations<a href="#practical-considerations"><i class="fas fa-hashtag"></i></a></h3></h3><p>In practice, since the score $s(x_i, x_j)= q_i \cdot k_j$ can be very large ($+\infty$) or small ($-\infty$), it can cause overflow or underflow problem when taking the exponentials in calculating the normalized weight $\alpha_{ij}$. To mitigate this issue, one may divide the dot product by the square root of the dimensionality of the query and key vectors:</p>\[s(x_i, x_j) = \frac{q_i\cdot k_j}{\sqrt{d_k}}.\]<p>Due to the parallel nature of the model, we can concatenate all the input embeddings into a single matrix $X$, and multiply it by the key, query and value matrices to get matrices containing all the $k,q$ and $v$ vectors.</p>\[\begin{align} Q &amp;= W^QX\\ K &amp;= W^KX\\ V &amp;= W^VX. \end{align}\]<p>Finally, we can reduce the entire self-attention step for an entire sequence to the following computation:</p>\[\mathrm{SelfAttention}(X) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V.\]<p>This brings up a new problem though. The calculation of matrix product $QK^T$ results in a score $s(x_i,x_j)$ for each $x_i$ and each $x_j$ in input sequence $(x_1,\ldots,x_n)$, including those $x_j$ for $j&gt;i$. In language modeling, our task is to predict the next word, so we may only use $x_1,\ldots,x_i$ as inputs to produce output $y_i$. Using $x_{i+1},\ldots,x_n$ to predict $x_{i+1}$ would not lead to meaningful learning. To fix this, the elements in the upper-triangular portion of the comparisons matrix are set to $-\infty$, thus eliminating any knowledge of words that follow in the sequence.</p><h2 id="transformers">Transformers<a href="#transformers"><i class="fas fa-hashtag"></i></a></h2></h2><p>The Transformer <a href="#2">[2]</a> model (Figure 3) is built on top of the Attention model, to acheive more potentials.</p><ol><li><p>First, in the transformer block, input is added to the output from the self-attention layer and normalized. Then it goes through a feedforward layer and then again residual connection and normalization. The Transformer block can be stacked and combined with other layers, to form a final model.</p><li><p>The self-attention layer in Figure 3 can be replaced with multi-head attention layer (Figure 4), in which you map an input to several self-attention layers in parallel, each with its own parameters, and then concatenate them and map the result to an output with the original dimension. The motivation for this is to let those self-attention layers capture different relationships among the inputs $x_1,\ldots,x_n$.</p><li><p>Finally, to further capture the information about the sequential order of the inputs, some positional embeddings may be added to the input sequence.</p></ol><p><img data-proofer-ignore data-src="/assets/imgs/transformers/transformer.png" alt="Transformer block" /> <em>Figure 3: Transformer block</em></p><p><img data-proofer-ignore data-src="/assets/imgs/transformers/multihead-attention.png" alt="Multi-head attention" /> <em>Figure 4: Multi-head attention</em></p><h3 id="generative-pre-training-gpt">Generative Pre-Training (GPT)<a href="#generative-pre-training-gpt"><i class="fas fa-hashtag"></i></a></h3></h3><p>The GPT <a href="#3">[3]</a> model was proposed by OpenAI in 2018. It is a Transformer model with <strong>generative pretraining</strong> and <strong>discriminative fine-tuning</strong>. Namely, it first trains on unlabeled text data to predict the next word, with NLL as the loss. Then for downstream tasks, you add a final output layer to the model, and you train on labeled data (with e.g. NLL as loss) to fine-tune the parameters of the model.</p><p>The point is to take advantage of the large amount of cheap unlabeled data available in the wild, and to reduce the reliance on human-labeled data. The <a href="https://openai.com/blog/better-language-models/">GPT-2</a> <a href="#4">[4]</a> model architecture is the same with GPT, just much larger. The authors examined performance of the model on discriminative tasks without the fine-tuning step, which is called ‚Äúzero-shot‚Äù.</p><h3 id="bert">BERT<a href="#bert"><i class="fas fa-hashtag"></i></a></h3></h3><p>BERT <a href="#5">[5]</a>, which stands for ‚ÄúBidirectional Encoder Representations from Transformers‚Äù, is built upon GPT, with two improvements.</p><ol><li>It uses <strong>bidirectional Transformer</strong> blocks, rather than unidirectional Transformer blocks used in GPT (i.e. Figure 3). Namely, each unit in Figure 1 connects not only to previous and current words, but to all the input words. The reason for this is that, by establishing more direct connections to the inputs, the model could have more capacity.<li><p>Pretraining consists of <strong>two tasks</strong>:</p><p>(1) masked language modeling (Masked LM), where some random tokens are masked and the model learns to predict those tokens. This is to avoid failure of plain language modeling, since in such setting the bi-Transformer block can see the word it is going to predict, so training with such objective leads to collapse, as we have discussed earlier.</p><p>(2) next sentence prediction (NSP). This is a binary task. Given two sentences $A$ and $B$, predict if $B$ is the next sentence that follows $A$. The purpose is to train a model that understands sentence relationships, so that it can have better performance in downstream tasks like Question Answering (QA) and Natural Language Inference (NLI).</p></ol><p>To get a more detailed sense of the BERT model, you can read the following quote from the paper <a href="#5">[5]</a>:</p><blockquote><p>To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as ‚Äúsentences‚Äù even though they are typically much longer than single sentences (but can be shorter also). The first sentence receives the $\texttt{A}$ embedding and the second receives the $\texttt{B}$ embedding. $50\%$ of the time $\texttt{B}$ is the actual next sentence that follows $\texttt{A}$ and $50\%$ of the time it is a random sentence, which is done for the ‚Äúnext sentence prediction‚Äù task. They are sampled such that the combined length is $\leq512$ tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of $15\%$, and no special consideration given to partial word pieces.</p><p>We train with batch size of $256$ sequences ($256$ sequences $*$ $512$ tokens = 128,000 tokens/batch) for 1 million steps, which is approximately $40$ epochs over the $3.3$ billion word corpus. We use Adam with learning rate of 1e-4, $\beta_1=0.9$, $\beta_2=0.999$, L2 weight decay of $0.01$, learning rate warmup over the first $10,000$ steps, and linear decay of the learning rate. We use a dropout probability of $0.1$ on all layers. We use a $\texttt{gelu}$ activation rather than the standard $\texttt{relu}$, following OpenAI GPT. The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.</p><p>For fine-tuning, most model hyperparameters are the same as in pre-training, with the exception of batch size ($16,32$), learning rate (e.g. 5e-5), and number of training epochs ($2,3,4$).</p><p>‚Ä¶‚Ä¶ <strong>The core argument of this work is that the bi-directionality and the two pre-training tasks account for the majority of the empirical improvements</strong>.</p></blockquote><h2 id="summary--whats-next">Summary &amp; What‚Äôs Next<a href="#summary--whats-next"><i class="fas fa-hashtag"></i></a></h2></h2><p>So here we are. We have studied all the common models in NLP up to now, from the simplest <a href="/posts/nlp-prob-ngram/">n-gram</a> model, to <a href="/posts/embedding-and-word2vec/">word2vec</a>, <a href="https://lifei.tech/posts/sequence-labeling/#hidden-markov-model-hmm">Hidden Markov Model</a>, <a href="/posts/sequence-labeling/#conditional-random-field-crf">Conditional Random Field</a>, <a href="/posts/rnn/">recurrent neural networks</a>, to Transformer-based models like GPT and BERT in this post. Transformer models achieve better results because more direct connections between inputs and outputs lead to more efficient learning. Not only are they the state-of-the-art models in NLP, but computer vision, for which convolutional neural networks were once dominant, also sees the trend of transitioning to Transformer-based models. See e.g. ViT <a href="#6">[6]</a> and MAE <a href="#7">[7]</a>.</p><p>Another noticeable trend in AI research is that, big companies are training ever bigger models with billions of parameters. See e.g. Microsoft‚Äôs Florence <a href="#8">[8]</a> or Google‚Äôs Pathways <a href="#9">[9]</a> model. The cost of training such models is astronomical, which often requires GPU resources that cost hundreds or thousands of millions of dollars. This is simply prohibitive for any individual. Gone are the days when someone can just sit in front of a desk and implement and run a state-of-the-art model on his or her laptop or on a rented machine in the cloud.</p><p>While pushing the boundary of neural network models by boosting model size is perfectly legitimate, there are many other important problems to consider. One fundamental problem is, how do neural network models actually work? What do they learn? What is the function of each parameter? Second, how should a model acquire robust generalization ability from less data, just as humans do? And is neural network really the ultimate paradigm for Artificial Intelligence? While current models may still have limited capacity, I‚Äôm optimistic that, we should see better and better AI models coming out in the future, and one day machines can have enough intelligence to handle tasks and jobs we do today.</p><hr /><p>Cite as:</p><div class="language-bibtex highlighter-rouge"><div class="code-header"> <span label-text="Bibtex"><i class="fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nc">@article</span><span class="p">{</span><span class="nl">lifei2022transformers</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">"Attention and Transformers"</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">"Li, Fei"</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">"https://lifeitech.github.io"</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">"2022"</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">"https://lifeitech.github.io/posts/attention-and-transformers/"</span>
<span class="p">}</span>
</pre></table></code></div></div><h2 id="references">References<a href="#references"><i class="fas fa-hashtag"></i></a></h2></h2><p><a id="1">[1]</a> Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. ‚ÄúNeural machine translation by jointly learning to align and translate.‚Äù <em>arXiv preprint arXiv:1409.0473</em> (2014).</p><p><a id="2">[2]</a> Vaswani, Ashish, et al. ‚ÄúAttention is all you need.‚Äù <em>Advances in neural information processing systems</em> 30 (2017).</p><p><a id="3">[3]</a> Radford, Alec, et al. ‚ÄúImproving language understanding by generative pre-training.‚Äù (2018).</p><p><a id="4">[4]</a> Radford, Alec, et al. ‚ÄúLanguage models are unsupervised multitask learners.‚Äù <em>OpenAI blog 1.8</em> (2019): 9.</p><p><a id="5">[5]</a> Devlin, Jacob, et al. ‚ÄúBert: Pre-training of deep bidirectional transformers for language understanding.‚Äù <em>arXiv preprint arXiv:1810.04805</em> (2018).</p><p><a id="6">[6]</a> Dosovitskiy, Alexey, et al. ‚ÄúAn image is worth 16x16 words: Transformers for image recognition at scale.‚Äù <em>arXiv preprint arXiv:2010.11929</em> (2020).</p><p><a id="7">[7]</a> He, Kaiming, et al. ‚ÄúMasked autoencoders are scalable vision learners.‚Äù <em>arXiv preprint arXiv:2111.06377</em> (2021).</p><p><a id="8">[8]</a> Yuan, Lu, et al. ‚ÄúFlorence: A New Foundation Model for Computer Vision.‚Äù <em>arXiv preprint arXiv:2111.11432</em> (2021).</p><p><a id="9">[9]</a> Chowdhery, Aakanksha, et al. ‚ÄúPaLM: Scaling Language Modeling with Pathways.‚Äù <em>arXiv preprint arXiv:2204.02311</em> (2022).</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/data-science/'>Data Science</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/attention/" class="post-tag no-text-decoration" >attention</a> <a href="/tags/transformer/" class="post-tag no-text-decoration" >transformer</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Attention and Transformers - lifeitech.github.io&url=https://lifeitech.github.io/posts/attention-and-transformers/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Attention and Transformers - lifeitech.github.io&u=https://lifeitech.github.io/posts/attention-and-transformers/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Attention and Transformers - lifeitech.github.io&url=https://lifeitech.github.io/posts/attention-and-transformers/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://lifeitech.github.io/posts/attention-and-transformers/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" class="js-toc"></nav></div><script src="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js"></script> <script> tocbot.init({ tocSelector: '.js-toc', contentSelector: '.js-toc-content', headingSelector: 'h1, h2, h3', hasInnerContainers: true, }); </script></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/sequence-labeling/"><div class="card-body"> <em class="timeago small" date="2022-03-26 09:00:00 +0800" >Mar 26, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Sequence Labeling with HMM and CRF</h3><div class="text-muted small"><p> Sequence labeling is a classical task in natural language processing. In this task, a program is expected to recognize certain information given a piece of text. This is challenging, because eve...</p></div></div></a></div><div class="card"> <a href="/posts/rnn/"><div class="card-body"> <em class="timeago small" date="2022-04-08 22:00:00 +0800" >Apr 8, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RNN, LSTM and GRU</h3><div class="text-muted small"><p> In this post we introduce the recurrent neural network (RNN) model in natural language processing (NLP), as well as two improvements over it, long short-term memories (LSTM) and gated recurrent ...</p></div></div></a></div><div class="card"> <a href="/posts/nlp-prob-ngram/"><div class="card-body"> <em class="timeago small" date="2022-03-05 16:20:00 +0800" >Mar 5, 2022</em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Natural Language Processing, Probabilities and the n-gram Model</h3><div class="text-muted small"><p> This is the first one of a series of posts on Natural Language Processing (NLP). We give an introduction to the subject, mention two model evaluation methods, see how to assign probabilities to ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/rnn/" class="btn btn-outline-primary" prompt="Older"><p>RNN, LSTM and GRU</p></a> <a href="/posts/reinforce-algorithm/" class="btn btn-outline-primary" prompt="Newer"><p>The REINFORCE Algorithm</p></a></div><div id="disqus_thread" class="pt-2 pb-2"><p class="text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://lifeitech.github.io/posts/attention-and-transformers/'; this.page.identifier = '/posts/attention-and-transformers/'; }; /* Lazy loading */ var disqus_observer = new IntersectionObserver(function (entries) { if(entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://www-lifei-tech.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] }); disqus_observer.observe(document.querySelector('#disqus_thread')); /* Auto switch theme */ function reloadDisqus() { /* Disqus hasn't been loaded */ if (typeof DISQUS === "undefined") { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } const modeToggle = document.querySelector(".mode-toggle"); if (typeof modeToggle !== "undefined") { /* modeToggle.addEventListener('click', reloadDisqus); // not pretty for 'color-scheme' */ window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', reloadDisqus); } </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> ¬© 2024 <a href="https://lifeitech.github.io">Fei Li</a> <span></span></p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/app-development/">app-development</a> <a class="post-tag" href="/tags/database/">database</a> <a class="post-tag" href="/tags/generative-model/">generative-model</a> <a class="post-tag" href="/tags/javascript/">javascript</a> <a class="post-tag" href="/tags/math/">math</a> <a class="post-tag" href="/tags/react/">react</a> <a class="post-tag" href="/tags/web-development/">web-development</a> <a class="post-tag" href="/tags/attention/">attention</a> <a class="post-tag" href="/tags/big-data/">big-data</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdnjs.cloudflare.com/ajax/libs/simple-jekyll-search/1.9.2/simple-jekyll-search.min.js" referrerpolicy="no-referrer"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js" integrity="sha512-IsNh5E3eYy3tr/JiX2Yx4vsCujtkhwl7SLqgnwLNgf04Hrt9BT9SXlLlZlWx+OK4ndzAoALhsMNcCmkggjZB1w==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { tags: 'ams', inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ], processEscapes: true } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.1/es5/tex-mml-chtml.min.js" integrity="sha512-lt3EkmQb16BgAXR0iCk+JUJyDFmS9NZEMXCXK169qQoWcXu9CS4feejtxkjjUruw/Y0XfL1qxh41xVQPvCxM1A==" crossorigin="anonymous" referrerpolicy="no-referrer"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.1.3/js/bootstrap.bundle.min.js" integrity="sha512-pax4MlgXjHEPfCwcJLQhigY7+N8rt6bVvWLFyUMuxShv170X53TRzGPmPkZmGBhk+jikR8WBM4yl7A9WMHHqvg==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
